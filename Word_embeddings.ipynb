{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Natrual Lanuage Processing (NLP) using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_md\") \n",
    "doc = nlp(\"The big grey dog ate all of the chocolate, but fortunately he wasn't sick!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'big',\n",
       " 'grey',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chocolate,',\n",
       " 'but',\n",
       " 'fortunately',\n",
       " 'he',\n",
       " \"wasn't\",\n",
       " 'sick!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'big',\n",
       " 'grey',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chocolate',\n",
       " ',',\n",
       " 'but',\n",
       " 'fortunately',\n",
       " 'he',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'sick',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'big',\n",
       " 'grey',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chocolate',\n",
       " 'but',\n",
       " 'fortunately',\n",
       " 'he',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'sick']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in doc if not token.is_punct | token.is_stop] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice', 'practice', 'practice']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practice = \"practice practiced practicing\" \n",
    "nlp_practice = nlp(practice) \n",
    "[word.lemma_ for word in nlp_practice] \n",
    "['practice', 'practice', 'practice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Barack Obama, 'PERSON'),\n",
       " (American, 'NORP'),\n",
       " (44th, 'ORDINAL'),\n",
       " (the United States, 'GPE'),\n",
       " (2009, 'DATE'),\n",
       " (2017, 'CARDINAL'),\n",
       " (first, 'ORDINAL'),\n",
       " (African American, 'NORP'),\n",
       " (first, 'ORDINAL'),\n",
       " (United States, 'GPE')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obama = \"\"\"Barack Obama is an American politician who served as ...: the 44th President of the United States from 2009 to 2017. He is the first ...: African American to have served as president, ...: as well as the first born outside the contiguous United States.\"\"\" \n",
    "nlp_obama = nlp(wiki_obama)\n",
    "[(i, i.label_) for i in nlp_obama.ents] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Yesterday, 'NN'),\n",
       " (Germany, 'NNP'),\n",
       " (won, 'VBD'),\n",
       " (the, 'DT'),\n",
       " (world, 'NN'),\n",
       " (cup, 'NN'),\n",
       " (again, 'RB'),\n",
       " (now, 'RB'),\n",
       " (the, 'DT'),\n",
       " (sixth, 'JJ'),\n",
       " (time, 'NN'),\n",
       " (in, 'IN'),\n",
       " (a, 'DT'),\n",
       " (row, 'NN'),\n",
       " (., '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(\"Yesterday Germany won the world cup again now the sixth time in a row.\") \n",
    "pos_tags = [(i, i.tag_) for i in doc2]\n",
    "pos_tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between vectors\n",
    "\n",
    "To measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$\n",
    "\n",
    "where $u.v$ is the dot product (or inner product) of two vectors, $||u||_2$ is the norm (or length) of the vector $u$, and $\\theta$ is the angle between $u$ and $v$. This similarity depends on the angle between $u$ and $v$. If $u$ and $v$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. \n",
    "\n",
    "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king and queen: 0.7252610325813293\n",
      "italy and berlin: 0.5053220987319946\n",
      "italy and rome: 0.722023069858551\n",
      "italy and milano: 0.5679553151130676\n",
      "italy and munich: 0.5211115479469299\n",
      "italy and pasta: 0.334966242313385\n",
      "italy and pizza: 0.34625598788261414\n",
      "italy and burger: 0.1485363394021988\n",
      "0.365557\n",
      "0.4164321\n",
      "0.33112225\n",
      "0.33112225\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(vec1,vec2):\n",
    "    return np.dot(vec1.vector, vec2.vector) / (vec1.vector_norm * vec2.vector_norm)\n",
    "\n",
    "def cos_sim2(vec1,vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# => already builtin vector.similarity(othervector)\n",
    "\n",
    "# king and queen\n",
    "king = nlp.vocab['king']\n",
    "man = nlp.vocab['man']\n",
    "woman = nlp.vocab['woman']\n",
    "queen = nlp.vocab['queen']\n",
    "\n",
    "# countries\n",
    "germany = nlp.vocab['germany']\n",
    "italy = nlp.vocab['italy']\n",
    "usa = nlp.vocab['usa']\n",
    "\n",
    "#cities\n",
    "berlin = nlp.vocab['berlin']\n",
    "newyork = nlp.vocab['new york city']\n",
    "rome = nlp.vocab['rome']\n",
    "milano = nlp.vocab['milano']\n",
    "munich = nlp.vocab['munich']\n",
    "\n",
    "# food\n",
    "pizza = nlp.vocab['pizza']\n",
    "pasta = nlp.vocab['pasta']\n",
    "burger = nlp.vocab['burger']\n",
    "\n",
    "\n",
    "print(\"king and queen: {}\".format(king.similarity(queen)))\n",
    "print(\"italy and berlin: {}\".format(italy.similarity(berlin)))\n",
    "print(\"italy and rome: {}\".format(italy.similarity(rome)))\n",
    "print(\"italy and milano: {}\".format(italy.similarity(milano)))\n",
    "print(\"italy and munich: {}\".format(italy.similarity(munich)))\n",
    "\n",
    "print(\"italy and pasta: {}\".format(italy.similarity(pasta)))\n",
    "print(\"italy and pizza: {}\".format(italy.similarity(pizza)))\n",
    "print(\"italy and burger: {}\".format(italy.similarity(burger)))\n",
    "\n",
    "\n",
    "print(cos_sim2(germany.vector-berlin.vector,italy.vector-rome.vector))\n",
    "print(cos_sim2(germany.vector-berlin.vector,italy.vector-milano.vector))\n",
    "print(cos_sim2(germany.vector-berlin.vector,italy.vector-munich.vector))\n",
    "print(cos_sim2(germany.vector-berlin.vector,italy.vector-munich.vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kennel',\n",
       " 'dog',\n",
       " 'canine',\n",
       " 'hound',\n",
       " 'canines',\n",
       " 'dogs',\n",
       " 'puppy',\n",
       " 'poodle',\n",
       " 'terrier',\n",
       " 'husky']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:10]\n",
    " \n",
    "[w.lower_ for w in most_similar(nlp.vocab['dog'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word analogy\n",
    "In the word analogy task, we complete the sentence <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. An example is <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. In detail, we are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \\approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['germany',\n",
       " 'Germany',\n",
       " 'GERMANY',\n",
       " 'Rhine',\n",
       " 'Salzburg',\n",
       " 'lech',\n",
       " 'livigno',\n",
       " 'deutschland',\n",
       " 'Styria',\n",
       " 'Innsbruck']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a is to be as c is to d\n",
    "# find d\n",
    "\n",
    "a = nlp(\"germany\")\n",
    "b = nlp(\"soccer\")\n",
    "c = nlp(\"usa\")\n",
    "\n",
    "# b - a = d - c\n",
    "\n",
    "def most_similar():\n",
    "    queries = [w for w in nlp.vocab]\n",
    "    by_similarity = sorted(queries, key=lambda w: cos_sim2(b.vector-a.vector, c.vector - w.vector), reverse=True)\n",
    "    return by_similarity[:10]\n",
    "\n",
    "[w.orth_ for w in most_similar()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
